# -*- coding: utf-8 -*-
"""Car.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6vrV8DJCJ7tVFxhgjabIe8LJCDmK8Vs
"""

import pandas as pd
import numpy as np

eps = np.finfo(float).eps

from numpy import log2 as log
carData = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data')
carData.head()

#len(carData)
#carData.shape
#carData.tail
carData.info

carData.describe(include=np.object)



carData.columns = ['buying','maintanance','doors','persons','boot','safety','acceptability']
carData.head()

def findEntropy(df):
    #we create a function to calculate entropy
    class1 = carData.keys()[-1] 
    entropy = 0
    values = df[class1].unique()
    for value in values:
        fraction = df[class1].value_counts()[value]/len(df[class1])
        entropy += -fraction*np.log2(fraction)
    return entropy

def findEntropyAttribute(carData,attribute):
  #create function to calculate entropy of all attributes
  class1 = carData.keys()[-1]
  targetVariable = carData[class1].unique()
  variable = carData[attribute].unique()
  entropy2 = 0
  for variable in variable:
    entropy = 0
    for targetVariable in targetVariable:
        num = len(carData[attribute][carData[attribute]==variable][carData[class1] ==targetVariable])
        den = len(carData[attribute][carData[attribute]==variable])
        fraction = num/(den+eps)
        entropy += -fraction*log(fraction+eps)
        fraction2 = den/len(carData)
        entropy2 += -fraction2*entropy
    return abs(entropy2)

def findResult(carData):
    #function to find highest information gain (IG)
    Entropy_att = []
    IG = []
    for key in carData.keys()[:-1]:
#Entropy_att.append(findEntropyAttribute(carData,key))
        IG.append(findEntropy(carData)-findEntropyAttribute(carData,key))
    return carData.keys()[:-1][np.argmax(IG)]

def getSubtable(carData, node, value):
    #Function to get subtable of the met conditions
    #node: column name
    #value: value of column
    return carData[carData[node] == value].reset_index(drop=True)

def buildTree(carData,tree=None): 
    #function to build ID3 decision tree
    class1 = carData.keys()[-1]  
    #Here we build our decision tree

    #Get attribute with maximum information gain (IG)
    node = findResult(carData)
    
    #Get distinct value of that attribute 
    attValue = np.unique(carData[node])
    
   
    if tree is None:
      tree={}
      tree[node] = {}
    
   #We make loop to construct a tree by calling this function recursively. 
    #In this we check if the subset is pure and stops if it is pure. 

    for value in attValue:
        
        subtable = getSubtable(carData,node,value)
        clValue,counts = np.unique(subtable['acceptability'],return_counts=True)                        
        
        if len(counts)==1:#Checking purity of subset
            tree[node][value] = clValue[0]                                                    
        else:        
            tree[node][value] = buildTree(subtable) #Calling the function recursively 
                   
    return tree

tree = buildTree(carData)

import pprint
pprint.pprint(tree)

def predict(inst,tree):
   #function to predict for any input variables
    #Recursively we go through the tree that we built earlier

    for nodes in tree.keys():        
        
        value = inst[nodes]
        tree = tree[nodes][value]
        prediction = 0
            
        if type(tree) is dict:
            prediction = predict(inst, tree)
        else:
            prediction = tree
            break;                            
        
    return prediction

data = {'buying':'vhigh','maintanance':'low','doors':"4",'persons':'4','boot':'big','safety':'high'}

inst = pd.Series(data)
prediction = predict(inst,tree)
prediction